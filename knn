import numpy as np
import random
import matplotlib.pyplot as plt
from collections import Counter

# Function to load Iris dataset from a CSV file, skipping the header
def load_iris_dataset(file_path):
    dataset = []
    with open(file_path, 'r') as file:
        next(file)  # Skip the header row
        for line in file:
            if line.strip():
                row = line.strip().split(',')
                features = [float(val) for val in row[1:-1]]  # Skip 'Id' column, convert feature values to float
                label = row[-1]  # Class label remains as string
                dataset.append(features + [label])
    return dataset

# Function to calculate Euclidean distance
def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))

# K-NN Classifier
def knn_classifier(train_data, test_sample, k):
    distances = []
    
    # Calculate distance between the test sample and all training samples
    for data in train_data:
        distance = euclidean_distance(np.array(data[:-1]), np.array(test_sample[:-1]))
        distances.append((distance, data[-1]))  # store the distance and the label

    # Sort by distance and get top k nearest neighbors
    distances.sort(key=lambda x: x[0])
    k_nearest_neighbors = distances[:k]
    
    # Get the most common class (vote)
    labels = [neighbor[1] for neighbor in k_nearest_neighbors]
    most_common_label = Counter(labels).most_common(1)[0][0]
    
    return most_common_label

# Function to split data into training and testing sets
def train_test_split(data, test_size=0.2):
    random.shuffle(data)
    split_index = int(len(data) * (1 - test_size))
    return data[:split_index], data[split_index:]

# Function to calculate accuracy
def calculate_accuracy(y_true, y_pred):
    correct = np.sum(np.array(y_true) == np.array(y_pred))
    return correct / len(y_true)

# Confusion matrix
def confusion_matrix(y_true, y_pred, labels):
    matrix = np.zeros((len(labels), len(labels)), dtype=int)
    label_to_index = {label: i for i, label in enumerate(labels)}
    for true_label, pred_label in zip(y_true, y_pred):
        matrix[label_to_index[true_label]][label_to_index[pred_label]] += 1
    return matrix

# Main function to run K-NN and evaluate performance
def knn_evaluate(dataset, k_values, test_size=0.2):
    train_data, test_data = train_test_split(dataset, test_size)
    x_test = [row[:-1] for row in test_data]
    y_test = [row[-1] for row in test_data]

    accuracies = []
    for k in k_values:
        y_pred = []
        for test_sample in test_data:
            predicted_label = knn_classifier(train_data, test_sample, k)
            y_pred.append(predicted_label)
        
        accuracy = calculate_accuracy(y_test, y_pred)
        accuracies.append(accuracy)
        
        # Print results for the maximum k value
        if k == max(k_values):
            print(f"Accuracy for k={k}: {accuracy * 100:.2f}%")
            print("Confusion Matrix:")
            print(confusion_matrix(y_test, y_pred, list(set(y_test))))
    
    return accuracies

# Plotting the accuracy vs k curve
def plot_k_vs_accuracy(k_values, accuracies):
    plt.figure(figsize=(8, 6))
    plt.plot(k_values, accuracies, marker='o', linestyle='-', color='b')
    plt.title('K vs Accuracy')
    plt.xlabel('K (Number of Neighbors)')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.show()

# Load the Iris dataset from the file 'Iris.csv'
dataset = load_iris_dataset('iris.csv')  # Path to your iris dataset file

# Define range of k values to test
k_values = list(range(1, 10))

# Evaluate the classifier
accuracies = knn_evaluate(dataset, k_values)

# Plot k vs accuracy
plot_k_vs_accuracy(k_values, accuracies)
